{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T13:33:59.472748Z",
     "start_time": "2025-02-06T13:33:59.458081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import whisper\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "\n",
    "# Check if GPU is available\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "GPU count: 1\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T13:34:01.403901Z",
     "start_time": "2025-02-06T13:33:59.493904Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(\"Nexdata/English_Emotional_Speech_Data_by_Microphone\", split=\"train\", streaming=True)",
   "id": "c9ea48e135583d78",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cd4d2f478934642984151caabebf779"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T13:34:01.418909Z",
     "start_time": "2025-02-06T13:34:01.412942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Function to convert audio to mp3\"\"\"\n",
    "# for i, audio in enumerate(dataset):\n",
    "#     # Step 1: Extract audio data\n",
    "#     audio_array = audio['audio']['array']\n",
    "#     sampling_rate = audio['audio']['sampling_rate']\n",
    "#\n",
    "#     # Step 2: Extract audio data from the dataset\n",
    "#     audio_array = np.array(audio_array)\n",
    "#\n",
    "#     # Step 3: Save as WAV first\n",
    "#     sf.write(f\"audio/audio_{i}.wav\", audio_array, sampling_rate)\n",
    "#\n",
    "#     # Step 4: Convert and save the file as mp3\n",
    "#     audio = AudioSegment.from_wav(f\"audio/audio_{i}.wav\")\n",
    "#     audio.export(os.path.join(path, f\"audio_{i}.mp3\"), format=\"mp3\")\n",
    "#     print('File saved!')\n",
    "#\n",
    "#     if i == 4:\n",
    "#         break"
   ],
   "id": "bdaa9f10eb88e74c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Function to convert audio to mp3'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T13:34:02.075106Z",
     "start_time": "2025-02-06T13:34:01.427073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = whisper.load_model(\"tiny\", device=device)\n",
    "print(model)"
   ],
   "id": "abb61142454b9731",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper(\n",
      "  (encoder): AudioEncoder(\n",
      "    (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (blocks): ModuleList(\n",
      "      (0-3): 4 x ResidualAttentionBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TextDecoder(\n",
      "    (token_embedding): Embedding(51865, 384)\n",
      "    (blocks): ModuleList(\n",
      "      (0-3): 4 x ResidualAttentionBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (cross_attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (cross_attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T13:34:02.095406Z",
     "start_time": "2025-02-06T13:34:02.083752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path_for_texts = r\"D:\\Users\\mlaudan\\PycharmProjects\\WhisperTask\\texts_from_mp3\"\n",
    "path_for_audio = r\"D:\\Users\\mlaudan\\PycharmProjects\\WhisperTask\\audio_mp3\"\n",
    "\n",
    "def transcribe(audio_path):\n",
    "    result = model.transcribe(audio_path)\n",
    "    text = result[\"text\"]\n",
    "    return text\n",
    "\n",
    "\n",
    "def save_text(text, path, name):\n",
    "    output_path = os.path.join(path_for_texts, path)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    file = os.path.join(output_path, f\"{name}.txt\")\n",
    "\n",
    "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    print(f'Text saved!: \"{text}\"')\n",
    "\n",
    "    return True\n",
    "\n"
   ],
   "id": "787a6d68c9fbecfd",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T13:34:35.971735Z",
     "start_time": "2025-02-06T13:34:33.302143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for root, dirs, files in os.walk(path_for_audio):\n",
    "    # print(f\"Path: {root}, Files: {files}\")\n",
    "    for i in files:\n",
    "        text = transcribe(os.path.join(root, i))\n",
    "        save_text(text, os.path.basename(root), i)\n",
    "\n"
   ],
   "id": "132e55511d49bd13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text saved!: \" This game is amazing.\"\n",
      "Text saved!: \" After the accident, Bill's sister is finally feeling better.\"\n",
      "Text saved!: \" I'm so happy to hear the good news!\"\n",
      "Text saved!: \" She's so cute.\"\n",
      "Text saved!: \" I'm so happy, haha, it's over now.\"\n"
     ]
    }
   ],
   "execution_count": 65
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
